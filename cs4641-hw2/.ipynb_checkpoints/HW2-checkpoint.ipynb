{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xs9yYSRW_Mvs"
   },
   "source": [
    "## Fall 2025 CS4641/CS7641 Homework 2 - Programming Section\n",
    "\n",
    "## Instructor: Dr. Max Mahdi Roozbahani, Dr. Nimisha Roy\n",
    "\n",
    "## Deadline: Friday, October 17th, 11:59 pm ET\n",
    "\n",
    "<!-- No changes needed on the below section -->\n",
    "* No unapproved extension of the deadline is allowed. For late submissions, please refer to the course website.\n",
    "\n",
    "* Discussion is encouraged on Ed as part of the Q/A. We encourage whiteboard-level discussions about the homework. **However, all assignments should be done individually.**\n",
    "\n",
    "* <font color='darkred'>Plagiarism is a _serious offense_. **You are responsible for completing your own work.**</font> You are not allowed to copy and paste, or paraphrase, or submit materials created or published by others, as if you created the materials. All materials submitted must be your own, and you must not collaborate with anyone or share your HW content except the ML instructional team.\n",
    "\n",
    "* <font color='darkred'>Working with a generative-AI platform _may constitute plagiarism_.</font> In line with the Joyner Heuristic being used by many classes at GT, you should treat collaboration with generative-AI as collaboration with a knowledgeable peer. <font color='darkred'>**Sharing the question or your work verbatim with an AI agent so as to generate an answer to the question is considered academic dishonesty and will be treated the same as any other incident of academic dishonesty.**</font> If you find yourself turning to generative-AI for help answering a question, we suggest that you instead make use of Ed or TA office hours.\n",
    "\n",
    "* <font color='darkred'>Even using generative-AI for formatting your answers is _not permitted_. **While we understand that LaTeX has a learning curve, you may not use any generative-AI platform to improve your writing or LaTeX formatting.**</font> These tools inherently produce output that may include a partial or complete solution to the question, including corrections to work you give it, even if purely prompted for syntactic use. Additionally, you have no custody over the data you give these platforms, which may leak or be used to inform subsequent training. Thus, while you are more than welcome to ask it about LaTeX commands and formatting tips, sharing the question or your answer to a question verbatim with an AI agent, even purely for syntactic use, is considered academic dishonesty and will be treated the same as any other incident of academic dishonesty. If you find yourself turning to generative-AI for help rewording your work to improve the language therein, remember that many of these questions will not be graded on language, but the content of your work. If you wish to improve it nonetheless, you can make use of the [Georgia Tech Communications Lab](https://www.communicationcenter.gatech.edu/) or TA office hours. If you find yourself turning to generative-AI for help reformatting your LaTeX, we suggest that you instead use the resources in the instructions below or TA office hours. Ed is also an appropriate place to ask about LaTeX formatting, so long as your post doesn't reveal answers to a question (or make a private post if your question necessitates revealing answers).\n",
    "\n",
    "* <font color='darkred'>_All_ incidents of suspected dishonesty, plagiarism, or violations of the Georgia Tech Honor Code will be subject to the institute’s Academic Integrity procedures. **If we observe any (even small) similarities/plagiarisms detected by Gradescope or our TAs, we will directly report the case to OSI, which may, unfortunately, lead to a very harsh outcome, pending review. Consequences can be severe, including academic probation or dismissal, grade penalties, a 0 grade for assignments concerned, and prohibition from withdrawing from the class.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uI6nZBMF_Mvw",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Instructions for the assignment\n",
    "\n",
    "<!-- No changes needed on the below section -->\n",
    "\n",
    "- This assignment consists of both programming and theory questions.\n",
    "\n",
    "- Unless a theory question explicitly states that no work is required to be shown, you must provide an explanation, justification, or calculation for your answer.\n",
    "\n",
    "- We will be using Gradescope for submission and grading of assignments.\n",
    "\n",
    "- **Unless a question explicitly states that no work is required to be shown, you must provide an explanation, justification, or calculation for your answer.** Basic arithmetic can be combined (it does not need to each have its own step); your work should be at a level of detail that a TA can follow it.\n",
    "\n",
    "- **For the \"Assignment 2 - Non-programming\" turn-in of this assignment, you will need to submit to Gradescope a PDF copy of your Jupyter Notebook with the cells ran.** Please refer to the Deliverables and Point Distribution section for an outline of the non-programming questions.\n",
    "\n",
    "- When submitting your assignment on Gradescope, **you are required to correctly map pages of your PDF to each question/subquestion to reflect where your solutions appear in your PDF.** <font color='darkred'>**Improperly mapped questions will receive a 0.**</font> You are permitted to submit a regrade request in this event; however, the review of such a request is at the sole discretion of the instructional staff and is not likely to be accepted.\n",
    "\n",
    "- <font color='darkred'>**When submitting to Gradescope, please make sure to mark the page(s) corresponding to each problem/sub-problem. The pages in the PDF should be of size 8.5\" x 11\" (landscape or portrait), otherwise there may be a deduction in points for oversized sheets, up to and including full credit deducted.**</font> Again, you are permitted to submit a regrade request in this event; however, the review of such a request is at the sole discretion of the instructional staff and is not likely to be accepted.\n",
    "\n",
    "- All assignments should be done individually; each student must write up and submit their own answers.\n",
    "\n",
    "- **Graduate Students**: You are required to complete any sections marked as Bonus for Undergrads.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7stIuyo_Mvx"
   },
   "source": [
    "## Using the autograder\n",
    "\n",
    "- Grads will find three assignments and Undergrads will find four assignments on Gradescope that correspond to HW2: \"Assignment 2 Programming\", \"Assignment 2 - Non-programming\", \"Assignment 2 Programming - Bonus for all\", and \"Assignment 2 Programming - Bonus for Undergrad\"\n",
    "\n",
    "<!-- No changes needed on the below section -->\n",
    "\n",
    "- You will submit your code for the autograder in the Assignment 2 Programming sections. Please refer to the Deliverables and Point Distribution section for what parts are considered required, bonus for undergrads, and bonus for all.\n",
    "\n",
    "- We provided you different .py files and we added libraries in those files please DO NOT remove those lines and add your code after those lines. Note that these are the only allowed libraries that you can use for the homework.\n",
    "\n",
    "- You are allowed to make as many submissions until the deadline as you like. Additionally, note that the autograder tests each function separately, therefore it can serve as a useful tool to help you debug your code if you are not sure of what part of your implementation might have an issue.\n",
    "<!-- No changes needed on the above section -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the local tests <a id='using_local_tests'></a>\n",
    "\n",
    "- For some of the programming questions we have included a local test using a small toy dataset to aid in debugging. The local tests are all stored in localtests.py\n",
    "- There are no points associated with passing or failing the local tests, you must still pass the autograder to get points.\n",
    "- **It is possible to fail the local test and pass the autograder** since the autograder has a certain allowed error tolerance while the local test allowed error may be smaller. Likewise, passing the local tests does not guarantee passing the autograder.\n",
    "- **You do not need to pass both local and autograder tests to get points, passing the Gradescope autograder is sufficient for credit.**\n",
    "- It might be helpful to comment out the tests for functions that have not been completed yet.\n",
    "- It is recommended to test the functions as it gets completed instead of completing the whole class and then testing. This may help in isolating errors. Do not solely rely on the local tests, continue to test on the autograder regularly as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Nz2030u_Mvy",
    "tags": []
   },
   "source": [
    "## Deliverables and Points Distribution\n",
    "\n",
    "### Q1: KMeans Clustering [45pts: 42pts + 3pts Grad / 1.5% Bonus for Undergrad]\n",
    "\n",
    "#### Deliverables: <font color = 'green'>kmeans.py</font>\n",
    "\n",
    "- **1.1 pairwise_dist** [5pts] - _programming_\n",
    "\n",
    "- **1.2 KMeans Implementation** [30pts: 27pts + 3pts Grad / 1.5% Bonus for Undergrad] - _programming_\n",
    "\n",
    "  - 1.2.1. init_centers [2pts]\n",
    "\n",
    "  - 1.2.2. kmpp_init [3pts Grad / 1.5% Bonus for Undergrad]\n",
    "\n",
    "  - 1.2.3. update_assignment [5pts]\n",
    "\n",
    "  - 1.2.4. update_centers [5pts]\n",
    "\n",
    "  - 1.2.5. get_loss [5pts]\n",
    "\n",
    "  - 1.2.6. train [10pts]\n",
    "\n",
    "- **1.3 Visualize KMeans** [0pts]\n",
    "\n",
    "- **1.4 Clustering Metrics** [10pts] - _programming_\n",
    "\n",
    "- **1.5 Limitation of KMeans** [0pts]\n",
    "\n",
    "### Q2: EM Algorithm [15pts + 1% Bonus for All]\n",
    "\n",
    "#### Deliverables: <font color = 'green'>Notebook Markdown Cell Text</font>\n",
    "\n",
    "- **2.1 Performing EM Update** [15pts] - _non-programming_\n",
    "\n",
    "  - 2.1.1 [3pts] - _non-programming_\n",
    "\n",
    "  - 2.1.2 [3pts] - _non-programming_\n",
    "\n",
    "  - 2.1.3 [9pts] - _non-programming_\n",
    "\n",
    "- **2.2 Gradient Descent and EM algorithm** [1% Bonus for All] - _non-programming_\n",
    "\n",
    "### Q3: GMM implementation [64pts + 2% Bonus for All]\n",
    "\n",
    "#### Deliverables: <font color = 'green'>gmm.py and Notebook Markdown Cell Text</font>\n",
    "\n",
    "- 3.1 Helper Functions [17pts] - _programming & non-programming_\n",
    "\n",
    "  - 3.1.1. softmax [5pts]\n",
    "\n",
    "  - 3.1.2. logsumexp [3pts + 4pts] - _programming & non-programming_\n",
    "\n",
    "  - 3.1.3. normalPDF [5pts] - _for CS4641 students only_\n",
    "\n",
    "  - 3.1.3. multinormalPDF [5pts] - _for CS7641 students only_\n",
    "\n",
    "- 3.2 GMM Implementation [30pts] - _programming_\n",
    "\n",
    "  - 3.2.1. init_components [5pts]\n",
    "\n",
    "  - 3.2.2.\\_ll_joint [10pts]\n",
    "\n",
    "  - 3.2.3. Setup iterative steps for EM algorithm [15pts]\n",
    "\n",
    "- 3.3 Image Compression and Pixel clustering [12pts] - _programming & non-programming_\n",
    "\n",
    "  - 3.3.1. GMM Clustering in the RGB Space [10pts]\n",
    "\n",
    "  - 3.3.2. Written Question [2pts]\n",
    "\n",
    "- 3.4 Compare Full Convariance Matrix with Diagonal Covariance Matrix [1% Bonus for All] - _non-programming_\n",
    "\n",
    "- 3.5 Generate samples from a Gaussian Mixture [5pts] - _non-programming_\n",
    "\n",
    "- 3.6 Random vs. KMeans Initialization [1% Bonus for All] - _non-programming_\n",
    "\n",
    "### Q4: Cleaning Super Duper Messy data with semi-supervised learning [7.0% Bonus for All]\n",
    "\n",
    "#### Deliverables: <font color = 'green'>semisupervised.py and Notebook Markdown Cell Text</font>\n",
    "\n",
    "- 4.1: KNN [2.8% Bonus for All] - _programming_\n",
    "\n",
    "  - 4.1.a. complete*, incomplete*, unlabeled\\_ [0.7% Bonus for All]\n",
    "\n",
    "  - 4.1.b. CleanData \\_\\_call\\_\\_ [1.4% Bonus for All]\n",
    "\n",
    "  - 4.1.c. MedianCleanData [0.7% Bonus for All]\n",
    "\n",
    "- 4.2: Getting acquainted with semi-supervised learning approaches [3.5% Bonus for All] - _programming & non-programming_\n",
    "\n",
    "  - 4.2.a. Write highlight summary [1.2% Bonus for All] - _non-programming_\n",
    "\n",
    "  - 4.2.b. Implement EM algorithm [2.3% Bonus for All] - _programming_\n",
    "\n",
    "- 4.3: Demonstrating the performance of the algorithm [0pts]\n",
    "\n",
    "- 4.4: Interpretation of Results [0.7% Bonus for All] - _non-programming_\n",
    "\n",
    "Note: It is highly recommended that you do Q4 (if not for the HW then before the project) as it teaches you imperfect data handling and a good understanding of how the models you have learnt can be used together for better results.\n",
    "\n",
    "### Q5: Hierarchical Clustering [9 pts Grad / 4.5% Bonus for Undergrad]\n",
    "\n",
    "#### Deliverables: <font color = 'green'>hierarchical_clustering.py</font>\n",
    "\n",
    "- 5.1 Hierarchical Clustering Implementation [9 pts Grad / 4.5% Bonus for Undergrad] - _programming_\n",
    "\n",
    "- 5.2 Hierarchical Clustering Visualization [0 pts]\n",
    "\n",
    "- 5.3 Hierarchical Clustering Large Dataset Visualization [0 pts]\n",
    "\n",
    "\n",
    "### Points Totals: ###\n",
    "- Total Base: 133 pts for grads / 121 pts for undergrads\n",
    "- Total Undergrad Bonus: 6%\n",
    "- Total Bonus for All: 10%\n",
    "\n",
    "### Gradescope Submission Deliverables: ###\n",
    "- For any Non-Programming portion: HW2.pdf\n",
    "  - run all cells\n",
    "  - use any utility to convert the Jupyter notebook to a pdf\n",
    "  - on some systems, it may be easier to print it as an html (which VSCode natively supports), then render that to a pdf\n",
    "- For 4641/7641 Programming Bonus for All: semisupervised.py\n",
    "- For 7641 Programming: kmeans.py, gmm.py, hierarchical_clustering.py\n",
    "- For 4641 Programming: kmeans.py, gmm.py\n",
    "- For 4641 Programming Bonus For Undergrad: kmeans.py, hierarchical_clustering.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKKsCsvo_Mvz"
   },
   "source": [
    "## 0 Set up\n",
    "\n",
    "This notebook is tested under [python 3.11.\\*\\*](https://www.python.org/downloads/release/python-3119/), and the corresponding packages can be downloaded from [miniconda](https://docs.conda.io/en/latest/miniconda.html). You may also want to get yourself familiar with several packages:\n",
    "\n",
    "- [jupyter notebook](https://jupyter-notebook.readthedocs.io/en/stable/)\n",
    "- [numpy](https://docs.scipy.org/doc/numpy-1.15.1/user/quickstart.html)\n",
    "- [matplotlib](https://matplotlib.org/users/pyplot_tutorial.html)\n",
    "\n",
    "You can create a python conda environment with the necessary packages using the instructions in the `environment/environment_setup.md` file.\n",
    "\n",
    "Please implement the functions that have \"raise NotImplementedError\", and after you finish the coding, please delete or comment \"raise NotImplementedError\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-15T02:21:19.417214Z",
     "iopub.status.busy": "2025-09-15T02:21:19.416583Z",
     "iopub.status.idle": "2025-09-15T02:21:43.399257Z",
     "shell.execute_reply": "2025-09-15T02:21:43.398531Z"
    },
    "id": "HhlrgV1__Mvz",
    "outputId": "1b21a9dc-5a27-4da2-c54c-400962b79bef"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "\n",
    "import localtests as localtests\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Version information\")\n",
    "\n",
    "print(\"python: {}\".format(sys.version))\n",
    "print(\"matplotlib: {}\".format(matplotlib.__version__))\n",
    "print(\"numpy: {}\".format(np.__version__))\n",
    "\n",
    "# Load image\n",
    "import imageio\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ysr89SO3_Mv0"
   },
   "source": [
    "## 1. KMeans Clustering [45pts total: 42pts + 3pts Grad / 1.5% Bonus for Undergrad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1B5DYD13_Mv2"
   },
   "source": [
    "KMeans is trying to solve the following optimization problem:\n",
    "\n",
    "\\begin{align}\n",
    "\\arg \\min_S \\sum_{i=1}^K \\sum_{x^{\\{j\\}} \\in S_i} ||x^{\\{j\\}} - \\mu_i||^2\n",
    "\\end{align}\n",
    "where one needs to partition the N observations into K clusters: $S = \\{S_1, S_2, \\ldots, S_K\\}$ and each cluster has $\\mu_i$ as its center.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtdX4lTz_Mv2"
   },
   "source": [
    "### 1.1 Pairwise Distance [5pts]\n",
    "\n",
    "In this section, you are asked to implement pairwise_dist function using the euclidean distance metric and the pairwise_dist_inf function using the infinity distance metric, also known as the chebyshev distance metric.\n",
    "\n",
    "Given $X \\in \\mathbb{R}^{N \\times D}$ and $Y \\in \\mathbb{R}^{M \\times D}$, obtain the pairwise distance matrix $dist \\in \\mathbb{R}^{N \\times M}$ using the euclidean distance metric, where $dist_{i, j} = ||X^{\\{i\\}} - Y^{\\{j\\}}||_2$. \n",
    "\n",
    "For highly dimensional datasets, the pairwise distance matrix using the $L_\\infty$ (chebyshev) distance metric can be less impacted by the curse of dimensionality compared to the euclidean distance metric, and is effective for grid based systems. Obtain the $L_\\infty$ (chebyshev) distance $dist \\in \\mathbb{R}^{N \\times M}$ using broadcasting, where $dist_{i, j} = ||X^{\\{i\\}} - Y^{\\{j\\}}||_\\infty = \\max^{d}_{k=1} |X^{\\{i\\}}_k - Y^{\\{j\\}}_k|$.\n",
    "\n",
    "DO NOT USE LOOPS in your implementation, **using for-loops or while-loops will result in 0 credit for this portion.**\n",
    "\n",
    "**Hint**: Use [array broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html), but your implementation shouldn't create a third dimension (which would timeout). This can be achieved by using the $X^2 + Y^2 – 2XY$ shortcut calculation for the euclidean distance metric. Also notice that **a numpy array in shape $(N,1)$ is NOT the same as that in shape $(N,)$** so be careful and consistent on what you are using. You can see the detailed explanation here. [Difference between numpy.array shape (R, 1) and (R,)](https://stackoverflow.com/questions/22053050/difference-between-numpy-array-shape-r-1-and-r)\n",
    "\n",
    "**Hint**: To calculate $X^2$ and $Y^2$ you can refer to the sum of squares function from assignment 1. For detailed explanation of pairwise distance function check this document - https://static.us.edusercontent.com/files/WLtSuk4PzW8e8M6VTsDq0BdJ\n",
    "\n",
    "We have provided some unit tests in localtests.py for you to check your implementation. See [Using the Local Tests](#using_local_tests) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:21:43.402711Z",
     "iopub.status.busy": "2025-09-15T02:21:43.402473Z",
     "iopub.status.idle": "2025-09-15T02:21:45.024633Z",
     "shell.execute_reply": "2025-09-15T02:21:45.023976Z"
    }
   },
   "outputs": [],
   "source": [
    "localtests.KMeansTests().test_pairwise_dist()\n",
    "localtests.KMeansTests().test_pairwise_speed()\n",
    "localtests.KMeansTests().test_pairwise_dist_inf()\n",
    "localtests.KMeansTests().test_pairwise_speed_inf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isP9lpcG_Mv4"
   },
   "source": [
    "### 1.2 KMeans Implementation [30pts: 27pts + 3pts Grad / 1.5% Bonus for Undergrad]\n",
    "\n",
    "In this section, you are asked to implement several methods in **kmeans.py**\n",
    "\n",
    "You may use [this visualization tool](http://tech.nitoyon.com/en/blog/2013/11/07/k-means/) to refine your understanding of KMeans.\n",
    "\n",
    "#### Initialization: [5pts: 2pts + 1.5% Bonus for Undergrad]\n",
    "\n",
    "The Kmeans algorithm is sensitive to how the centers are initialized and a bad initialization can increase the time required for convergence or may even converge to a non-optimal solution, but we need to pick somehow.\n",
    "\n",
    "#### 1.2.1. **init_centers** [2pts]\n",
    "\n",
    "As a naive approach, initialize the centers randomly by picking points from the dataset. Note that you should sample points without replacement/repetition, since we wouldn't want to pick the same point twice. This is actually a better approach than you might think. If there exists a large clustering of points in a region, and we're sampling all of the points uniformly, then there's a pretty good chance we'll pick a center in said region. The odds get worse with more clusters and more outliers, but it's a good place to start.\n",
    "\n",
    "In **kmeans.py**, implement **init_centers**.\n",
    "\n",
    "#### 1.2.2. **kmpp_init** [3pts Grad / 1.5% Bonus for Undergrad]\n",
    "\n",
    "One key intuition about ideal cluster centers is that a good starting point will have the centers be further away from each other.\n",
    "\n",
    "In **kmeans.py**, implement **kmpp_init**, per the instructions below.\n",
    "\n",
    "##### KMeans++\n",
    "\n",
    "The algorithm for KMPP that you will implement (slightly different than the procedure described in its original paper) can be described as follows:\n",
    "\n",
    "1. Sample 1% of the points from the dataset, uniformly at random (UAR) and without replacement. This sample will be the dataset the remainder of the algorithm uses to minimize initialization overhead.\n",
    "2. From the above sample, select only one random point to be the first cluster center.\n",
    "3. For each point in the sampled dataset, find the nearest cluster center and record the squared distance to get there.\n",
    "4. Examine all the squared distances and take the point with the maximum squared distance as a new cluster center. In other words, we will choose the next center based on the maximum of the minimum calculated distance instead of sampling randomly like in step 2. You may break ties arbitrarily.\n",
    "5. Repeat 3-4 until all k-centers have been assigned. You may use a loop over K to keep track of the data in each cluster.\n",
    "\n",
    "#### 1.2.3. Updating Cluster Assignments [5pts]\n",
    "\n",
    "After you've chosen your centers, you will need to update the membership of each point based on the closest center.\n",
    "You will implement this in **update_assignment**. See docstring for more details.\n",
    "\n",
    "#### 1.2.4. Updating Centers Assignments [5pts]\n",
    "\n",
    "Since cluster memberships may have changed, you will need to update the cluster centers.\n",
    "\n",
    "In **kmeans.py**, implement **update_centers**. See docstring for more details.\n",
    "\n",
    "**Hint**: You may use a loop over K to keep track of the data in each cluster, but avoid looping over N individual datapoints.\n",
    "\n",
    "#### 1.2.5. Loss & Convergence [5pts]\n",
    "\n",
    "We will consider KMeans to be converged when the change in loss drops below a threshold value. The loss will be defined as the sum of the squared distances between each point and its respective center. Keep this in mind when writing the **train** function.\n",
    "\n",
    "#### 1.2.6. Train the model [10pts]\n",
    "\n",
    "In the **train** method you will use all of the previously implemented steps to train your KMeans algorithm until convergence. Since the centers have already been initialized in the init function the general steps for the **train** method is as follows:\n",
    "\n",
    "1. Update the cluster assignment for each point\n",
    "2. Update the cluster centers based on the new assignments from Step 1\n",
    "3. Check to make sure there is no [mean without a cluster](https://www.youtube.com/watch?v=MAU0gQXn28s), i.e. no cluster center without any points assigned to it.\n",
    "   - In the event of a cluster with no points assigned, pick a random point in the dataset to be the new center and update your cluster assignment accordingly.\n",
    "4. Calculate the loss and check if the model has converged to break the loop early.\n",
    "   - The convergence criteria is measured by whether the percentage of difference in loss with respect to the previous iteration's loss is less than the given relative tolerance threshold (self.rel_tol).\n",
    "5. Iterate through steps 1 to 4 `max_iters` times. **Make sure to avoid infinite looping.**\n",
    "\n",
    "We have provided the following local tests to help you check your implementation. Provided unit-tests are meant as a guide and are not intended to be comprehensive. See [Using the Local Tests](#using_local_tests) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:21:45.027460Z",
     "iopub.status.busy": "2025-09-15T02:21:45.027222Z",
     "iopub.status.idle": "2025-09-15T02:21:45.626618Z",
     "shell.execute_reply": "2025-09-15T02:21:45.625987Z"
    }
   },
   "outputs": [],
   "source": [
    "localtests.KMeansTests().test_init()\n",
    "localtests.KMeansTests().test_update_centers()\n",
    "localtests.KMeansTests().test_kmeans_loss()\n",
    "localtests.KMeansTests().test_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Visualize KMeans [0pts]\n",
    "\n",
    "Cyber Sentinel Ava, a top fraud analyst in CyberHaven, is on a mission to protect the city’s financial network from cybercriminals. The Central Credit Network (CCN) has detected unusual transaction patterns—small, frequent payments flowing through dormant accounts—suggesting that compromised accounts are being exploited. Remembering her expertise in Machine Learning, Ava knows that anomaly detection using K-Means clustering can uncover hidden fraud rings. Your task is to assist Ava in deploying this AI-driven system to identify compromised accounts and stop a billion-credit heist before it’s too late\n",
    "\n",
    "\n",
    "All you need to do is run the next cell. It should output different plots of a subset of selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:21:45.629719Z",
     "iopub.status.busy": "2025-09-15T02:21:45.629363Z",
     "iopub.status.idle": "2025-09-15T02:21:47.142969Z",
     "shell.execute_reply": "2025-09-15T02:21:47.142155Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from utilities import *\n",
    "\n",
    "# Note that because of a different file structure, students' paths will be different\n",
    "data = pd.read_csv(\"./data/creditcard.csv\")\n",
    "X = data.iloc[:, data.columns != \"Class\"].to_numpy()\n",
    "k = 2\n",
    "\n",
    "create_plots(X, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Clustering Metrics [10pts]\n",
    "\n",
    "In this section, you will calculate metrics to assess the quality of your clustering algorithm. \n",
    "\n",
    "1. The **Adjusted Rand Index (ARI)** quantifies how consistently two clusterings group pairs of points, correcting for chance.\n",
    "2. The **Silhouette Coefficient** is a measure of how well a data point fits within its assigned cluster compared to other clusters.\n",
    "\n",
    "\n",
    "#### 1.4.1 Adjusted Rand Index (ARI) [3 points]\n",
    "\n",
    "As discussed in class, the computation for the Random Index is as follows:\n",
    "\n",
    "$$RI = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "Using this value, we can calculate the Adjusted Random Index score, which is simply the random index score adjusted for chance.\n",
    "\n",
    "$$ARI = \\frac{RI - \\mathbb{E}[\\text{RI}]}{max(RI) - \\mathbb{E}[\\text{RI}]}$$\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    "$$ max(RI) = 1 $$\n",
    "\n",
    "$$\\mathbb{E}[\\text{RI}] = \\frac{(TP+FP)(TP+FN) + (TN+FP)(TN+FN)}{(TP + TN + FP + FN)^2} $$\n",
    "\n",
    "\n",
    "Substituting these values into the $ARI$ formula, we see that\n",
    "\n",
    "$$ARI = \\frac{2(TP \\cdot TN - FP \\cdot FN)}{(TP + FN)(FN + TN) + (TP + FP)(FP + TN)}$$\n",
    "\n",
    "\n",
    "TP (True Positive) represents the number of data points that are correctly clustered together in both the algorithm's result and the ground truth.\n",
    "\n",
    "TN (True Negative) is the number of data points that are correctly placed in separate clusters in both the algorithm's result and the ground truth.\n",
    "\n",
    "FP (False Positive) counts the number of data points that are incorrectly clustered together in the algorithm's result but correctly separated in the ground truth.\n",
    "\n",
    "FN (False Negative) is the number of data points that are incorrectly separated in the algorithm's result but correctly clustered together in the ground truth.\n",
    "\n",
    "We have provided the following local tests to help you check your implementation. Provided unit-tests are meant as a guide and are not intended to be comprehensive. See [Using the Local Tests](#using_local_tests) for more details.\n",
    "\n",
    "#### 1.4.2 Silhouette Coefficient [5 points]\n",
    "\n",
    "As discussed in class, the computation for the Silhouette Coefficient is as follows:\n",
    "\n",
    "$$s(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), \\, b(i)\\}}$$\n",
    "\n",
    "where\n",
    "\n",
    "$a(i)$ = average distance of point i to all other points in its own cluster\n",
    "\n",
    "$b(i)$ = minimum average distance of point i to all points in any other cluster\n",
    "\n",
    "Thus, the overall silhouette coefficient can be computed as the average of the silhouette coefficient for all points:\n",
    "\n",
    "$$ S = \\frac{1}{N} \\sum_{i=1}^{N} s(i) $$\n",
    "\n",
    "**Refer to the class notes for more information on the Silhouette Coefficient**\n",
    "\n",
    "**Refer to the class notes and [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html) for more information on the Adjusted Random Index Measure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:21:47.157365Z",
     "iopub.status.busy": "2025-09-15T02:21:47.157009Z",
     "iopub.status.idle": "2025-09-15T02:21:47.203576Z",
     "shell.execute_reply": "2025-09-15T02:21:47.203042Z"
    }
   },
   "outputs": [],
   "source": [
    "localtests.KMeansTests().test_silhouette_score()\n",
    "localtests.KMeansTests().test_adjusted_rand_statistic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3 Comparison (Written Question) [2 points]\n",
    "\n",
    "Analyze the performance of these two evaluation metrics: Adjusted Rand Index (ARI) and Silhouette Coefficient (SC). Limit your response to 2-3 sentences MAX per question.\n",
    "\n",
    "a. Explain what each of these metrics measure. For each metric, describe what constitutes a “good” score and a “bad” score. Include the range of possible values (1 point)\n",
    "\n",
    "b. Identify the most notable difference between these two metrics and explain why this distinction is important when evaluating clustering results (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6RKWYnT_Mv6"
   },
   "source": [
    "### 1.5 Limitation of KMeans [0pts]\n",
    "\n",
    "You've now done the best you can selecting the perfect starting points and the right number of clusters. However, one of the limitations of K-Means Clustering is that it depends largely on the shape of the dataset. A common example of this is trying to cluster one circle within another (concentric circles). A K-means classifier will fail to do this and will end up effectively drawing a line that crosses the circles. You can visualize this limitation in the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:21:47.206794Z",
     "iopub.status.busy": "2025-09-15T02:21:47.206466Z",
     "iopub.status.idle": "2025-09-15T02:21:47.521696Z",
     "shell.execute_reply": "2025-09-15T02:21:47.520975Z"
    },
    "id": "_R0ckO9H_Mv6",
    "outputId": "5b7e8926-6161-4188-c476-21646664abfd"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# visualize limitation of kmeans\n",
    "from kmeans import *\n",
    "from sklearn.datasets import make_circles, make_moons\n",
    "\n",
    "X1, y1 = make_circles(factor=0.5, noise=0.05, n_samples=1500)\n",
    "X2, y2 = make_moons(noise=0.05, n_samples=1500)\n",
    "\n",
    "\n",
    "def visualise(\n",
    "    X, C, K=None\n",
    "):  # Visualization of clustering. You don't need to change this function\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=C, cmap=\"rainbow\")\n",
    "    if K:\n",
    "        plt.title(\"Visualization of K = \" + str(K), fontsize=15)\n",
    "    plt.show()\n",
    "    pass\n",
    "\n",
    "\n",
    "kmeans = KMeans(X1, 2)\n",
    "centers1, cluster_idx1, loss1 = kmeans.train()\n",
    "visualise(X1, cluster_idx1, 2)\n",
    "kmeans = KMeans(X2, 2)\n",
    "centers2, cluster_idx2, loss2 = kmeans.train()\n",
    "visualise(X2, cluster_idx2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnjRdvII_Mv7"
   },
   "source": [
    "# 2. EM algorithm [15pts + 1% Bonus for All]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEH3Rq6I_Mv7"
   },
   "source": [
    "### 2.1 Performing EM Update [15 pts]\n",
    "\n",
    "**ANSWERS CANNOT BE HANDWRITTEN**\n",
    "\n",
    "A univariate Gaussian Mixture Model (GMM) has two components, both of which have their own mean and standard deviation. The model is defined by the following parameters:\n",
    "$$ \\mathbf{z} \\sim Bernoulli(\\alpha) = \\begin{cases} \\alpha &\\text{if} \\, z=0 \\\\ 1-\\alpha &\\text{if} \\, z=1 \\\\\\end{cases}$$\n",
    "$$ p\\left(\\mathbf{x_n \\mid z=0}\\right) \\sim \\mathcal{N}(3\\upsilon, 4\\omega^{2}) $$\n",
    "$$ p\\left(\\mathbf{x_n \\mid z=1}\\right) \\sim \\mathcal{N}(2\\upsilon, 7\\omega^{2}) $$\n",
    "\n",
    "For a dataset of N datapoints, find the following:\n",
    "\n",
    "2.1.1. Write the marginal probability of x, i.e. $p(x)$. \\[3pts]\n",
    "<br> -- Express your answers in terms of $\\mathcal{N}(x|3\\upsilon, 4\\omega^{2})$ and $\\mathcal{N}(x|2\\upsilon, 7\\omega^{2})$ may be simpler.\n",
    "<br> -- HINT: For this question suppose we have a Gaussian Distribution $\\mathcal{N}(3\\upsilon, 4\\omega^{2})$, it means $\\mathcal{N}(\\mu = 3\\upsilon, \\sigma^{2} = 4\\omega^{2})$.\n",
    "<br> -- HINT: Start with the Sum Rule.\n",
    "\n",
    "\n",
    "2.1.2. E-Step: Compute the posterior probabilities, i.e, $p(z_0|x), p(z_1|x)$ \\[3pts]\n",
    "<br> -- Express your answers in terms of $\\mathcal{N}(x|3\\upsilon, 4\\omega^{2})$ and $\\mathcal{N}(x|2\\upsilon, 7\\omega^{2})$.\n",
    "<br> -- HINT: Try to apply Bayes Rule.\n",
    "\n",
    "2.1.3. M-Step: Compute the updated value of $\\omega^{2}$. (You can keep $\\mu$ fixed when you calculate the derivative.) \\[9pts]\n",
    "<br> -- Note that $\\omega^2$ is a shared variable between the two distributions, your final answer should be one equation including both Gaussian distributions.\n",
    "<br> -- Express your answers in terms of $\\tau$, $x$, and $\\upsilon$ (you will need to expand $\\mathcal{N}(3\\upsilon, 4\\omega^{2})$ and $\\mathcal{N}(2\\upsilon, 7\\omega^{2})$ into its PDF form).\n",
    "<br> -- HINT: Start from the below equation, note that $\\theta$ is shorthand for various variables, and take the derivative w.r.t. $\\omega^2$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\ell(\\theta|x) &= \\sum_n^{N} \\sum^{Z}_{k \\in \\{0, 1\\}}p(z_k \\mid x^{\\{n\\}},\\theta_{old})ln \\left[p(x^{\\{n\\}},z_k \\mid \\theta)\\right]\\\\\n",
    "\\ell(\\upsilon, \\omega^2, \\alpha \\mid x)\n",
    "&= \\sum_n^{N} \\sum^{Z}_{k \\in \\{0, 1\\}} p(z_k \\mid x^{\\{n\\}}, \\theta_{old}) \\ln\\left[p(x^{\\{n\\}}, z_k \\mid \\mu_k, \\sigma^2_k, \\alpha)\\right] \\\\\n",
    "&=  \\sum_n^{N} \\sum^{Z}_{k \\in \\{0, 1\\}} p(z_k \\mid x^{\\{n\\}}, \\theta_{old}) \\ln \\left[p(z_k \\mid \\alpha) p(x^{\\{n\\}} \\mid z_k, \\upsilon, \\omega^2)\\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Recall that $p(x^{\\{n\\}} \\mid z_k, \\upsilon, \\omega^2) \\rightarrow \\mathcal{N}(x^{\\{n\\}} \\mid \\mu_k, \\sigma_k)$ has been defined at the beginning of the problem.\n",
    "\n",
    "You can refer to [this lecture](https://mahdi-roozbahani.github.io/CS46417641-fall2025/course/04-probability-note.pdf) to gain an understanding of the EM Algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Gradient Ascent and EM algorithm [1% Bonus for All]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the computational advantage of using the EM algorithm compared to the Gradient Ascent algorithm for the problem presented in 2.1? Please provide your own qualitative analysis. [1% Bonus for All]\n",
    "<br> -- HINT: Think about the difference in updating parameters during each iteration. i.e., How many parameters are updated (with a step size) in gradient ascent each iteration? What do we do in each EM iteration (E-step then M-step) to simplify updates? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXcuY2pb_Mv7"
   },
   "source": [
    "## 3. GMM implementation [60pts total: 60pts + 2% Bonus for All]\n",
    "\n",
    "**Please make sure to read the problem setup in detail. Many questions for this section may have already been answered in the description and hints and docstrings.**\n",
    "\n",
    "A Gaussian Mixture Model (GMM) is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian Distribution. In a nutshell, GMM is a soft clustering algorithm in a sense that each data point is assigned to a cluster with a probability. In order to do that, we need to convert our clustering problem into an inference problem.\n",
    "\n",
    "Given $N$ samples $X = [x^{\\{1\\}}, x^{\\{2\\}}, \\ldots, x^{\\{N\\}}]^T$, where $x^{\\{i\\}} \\in \\mathbb{R}^D$. Let $\\pi$ be a K-dimensional probability density function and $(\\mu_k; \\Sigma_k)$ be the mean and covariance matrix of the $k^{th}$ Gaussian distribution in $\\mathbb{R}^D$.\n",
    "\n",
    "The GMM object implements EM algorithms for fitting the model and MLE for optimizing its parameters. It also has some particular hypothesis on how the data was generated:\n",
    "\n",
    "- Each data point $x^{\\{i\\}}$ is assigned to a cluster $k$ with probability of $\\pi_k$ where $\\sum_{k=1}^K \\pi_k = 1$\n",
    "- Each data point $x^{\\{i\\}}$ is generated from Multivariate Normal Distribution $\\cal{N}(\\mu_k, \\Sigma_k)$ where $\\mu_k \\in \\mathbb{R}^D$ and $\\Sigma_k \\in \\mathbb{R}^{D\\times D}$\n",
    "\n",
    "Our goal is to find a $K$-dimension Gaussian distributions to model our data $X$. This can be done by learning the parameters $\\pi, \\mu$ and $\\Sigma$ through likelihood function. Detailed derivation can be found in our slide of GMM. The log-likelihood function now becomes:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{ln } p(x^{\\{1\\}}, \\dots, x^{\\{N\\}} | \\pi, \\mu, \\Sigma) = \\sum_{i=1}^N \\text{ln } \\big( \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x^{\\{i\\}} | \\mu_k, \\Sigma_k)\\big)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLjk2ZYU_Mv8"
   },
   "source": [
    "From the lecture we know that MLEs for GMM all depend on each other and the responsibility $\\tau$. Thus, we need to use an iterative algorithm (the EM algorithm) to find the estimate of parameters that maximize our likelihood function. **All detailed derivations can be found in the lecture slide of GMM.**\n",
    "\n",
    "- **E-step:** Evaluate the responsibilities\n",
    "\n",
    "In this step, we need to calculate the responsibility $\\tau$, which is the conditional probability that a data point belongs to a specific cluster $k$ if we are given the datapoint, i.e. $P(z_k|x)$. The formula for $\\tau$ is given below:\n",
    "\n",
    "$$\n",
    "\\tau\\left(z_k\\right)=\\frac{\\pi_{k} \\cal{N}\\left(x | \\mu_{k}, \\Sigma_{k}\\right)}{\\sum_{j=1}^{K} \\pi_{j} \\cal{N}\\left(x | \\mu_{j}, \\Sigma_{j}\\right)}, \\quad \\text{for } k = 1, \\dots, K\n",
    "$$\n",
    "\n",
    "Note that each data point should have one probability for each component/cluster. For this homework, you will work with $\\tau\\left(z_k\\right)$ which has a size of $N\\times K$ and you should have all the responsibility values in one matrix.\n",
    "\n",
    "- **M-step:** Re-estimate Paramaters\n",
    "\n",
    "After we obtained the responsibility, we can find the update of parameters, which are given below:\n",
    "\n",
    "\\begin{align}\n",
    "\\mu_k^{new} &= \\dfrac{\\sum_{n=1}^N \\tau(z_k)x^{\\{n\\}}}{N_k} \\\\\n",
    "\\Sigma_k^{new} &= \\dfrac{1}{N_k}\\sum_{n=1}^N \\tau (z_k)^T(x^{\\{n\\}} - \\mu_k^{new})^T(x^{\\{n\\}}-\\mu_k^{new}) \\\\\n",
    "\\pi_k^{new} &= \\dfrac{N_k}{N}\n",
    "\\end{align}\n",
    "where $N_k = \\sum_{n=1}^N \\tau(z_k)$. Note that the updated value for $\\mu_k$ is used when updating $\\Sigma_k$. The multiplication of $\\tau (z_k)^T(x^{\\{n\\}} - \\mu_k^{new})^T$ is element-wise so it will preserve the dimensions of $(x^{\\{n\\}} - \\mu_k^{new})^T$.\n",
    "\n",
    "- We repeat E and M steps until the incremental improvement to the likelihood function is small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jhd5Z_53_Mv8"
   },
   "source": [
    "**Special Notes**\n",
    "\n",
    "- For undergraduate student: you may assume that the covariance matrix $\\Sigma$ is diagonal matrix, which means the features are independent. (i.e. the red intensity of a pixel is independent from its blue intensity, etc). Make sure you set **FULL_MATRIX = False** before you submit your code to Gradescope.\n",
    "- For graduate student: please assume full covariance matrix. Make sure you set **FULL_MATRIX = True** before you submit your code to Gradescope\n",
    "- Under numpy conventions, the class notes would have your dataset $X$ as $(D, N)$ but **the homework dataset is $(N, D)$ as mentioned on the instructions, so the formula is a little different from the lecture note in order to obtain the right dimensions of parameters.**\n",
    "\n",
    "**Hints**\n",
    "\n",
    "1. **DO NOT USE FOR LOOPS OVER N.** You can always find a way to avoid looping over the observation datapoints in our homework problem. If you have to loop over D or K, that is fine.\n",
    "\n",
    "2. You can initiate $\\pi_k$ the same for each $k$, i.e. $\\pi_k = \\frac{1}{K}, \\forall k = 1, 2, \\ldots, K$.\n",
    "\n",
    "3. In part 3 you are asked to generate the model for pixel clustering of image. We will need to use a multivariate Gaussian because each image will hava $N$ pixels and $D=3$ features which corresponds to red, green, and blue color intensities. It means that each image is a $(N\\times3)$ dataset matrix. In the following parts, remember $D=3$ in this problem.\n",
    "\n",
    "4. To avoid using for loops in your code, we recommend you take a look at the concept [Array Broadcasting in Numpy](https://numpy.org/doc/stable/user/basics.broadcasting.html). Also, certain calculations that required different shapes of arrays can also be achieved by broadcasting.\n",
    "\n",
    "5. Be careful of the dimensions of your parameters. Before you test anything on the autograder, please look at the instructions below on the shapes of the variables you need to output and how to format your return statement. Print the shape of an array by [print(array.shape)](https://www.w3schools.com/python/numpy/numpy_array_shape.asp) could enhance the functionality of your code and help you debugging. Also notice that **a numpy array in shape $(N,1)$ is NOT the same as that in shape $(N,)$** so be careful and consistent on what you are using. You can see the detailed explanation here. [Difference between numpy.array shape (R, 1) and (R,)](https://stackoverflow.com/questions/22053050/difference-between-numpy-array-shape-r-1-and-r)\n",
    "\n",
    "- The dataset $X$: $(N, D)$\n",
    "- $\\mu$: $(K, D)$.\n",
    "- $\\Sigma$: $(K, D, D)$\n",
    "- $\\tau$: $(N, K)$\n",
    "- $\\pi$: array of length $K$\n",
    "- ll_joint: $(N, K)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Iviglmb_Mv8"
   },
   "source": [
    "## 3.1 Helper functions [17pts]\n",
    "\n",
    "To facilitate some of the operations in the GMM implementation, we would like you to implement the following three helper functions. In these functions, \"logit\" refers to an input array of size $(N, D)$ that reperesents the unnormalized scores, that are passed to the softmax( ) or logsumexp( ) function. Remember the goal of helper functions is to facilitate our calculation so **DO NOT USE FOR LOOP OVER N**.\n",
    "\n",
    "### 3.1.1. softmax [5pts]\n",
    "\n",
    "Given $logit \\in \\mathbb{R}^{N \\times D}$, calculate $prob \\in \\mathbb{R}^{N \\times D}$, where $prob_{i, j} = \\frac{\\exp(logit_{i, j})}{\\sum_{d=1}^D exp(logit_{i, d})}$.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- $logit$ here refers to the unnormalized scores that are passed in as a parameter to the softmax function. The softmax operation normalizes these scores, resulting in them having values between 0 and 1. This allows us to interpret the normalized scores as a probability distribution over the classes.\n",
    "- It is possible that $logit_{i, j}$ is very large, making $\\exp(\\cdot)$ of it to explode. To make sure it is numerically stable, for each row of $logits$ subtract the maximum of that row.\n",
    "  - By property of Softmax equation, subtracting a constant value does not change the output. [Refer to Mathematical properties](https://en.wikipedia.org/wiki/Softmax_function#Mathematical_properties)\n",
    "\n",
    "**Special Notes**\n",
    "\n",
    "- Do not add back the maximum for each row.\n",
    "- Add **keepdims=True** in your np.sum() function to avoid broadcast error.\n",
    "\n",
    "### 3.1.2. logsumexp [3pts Programming + 4pts Written Questions]\n",
    "\n",
    "Given $logit \\in \\mathbb{R}^{N \\times D}$, calculate $s \\in \\mathbb{R}^N$, where $s_i = \\log \\big( \\sum_{j=1}^D \\exp(logit_{i, j}) \\big)$. Again, pay attention to the numerical problem. You may face similar conditions to the softmax function due to $logit_{i, j}$ being large. However, unlike softmax, subtracting the row maxes will affect the result. To adjust the output for correctness, after calculating the logsumexp of each row, you should add the maximum for each row of $logit$ back for your functions before returning the final value.\n",
    "\n",
    "**Special Notes**\n",
    "\n",
    "- This function is used in the call() function, which is given, and helps calculate the loss of log-likelihood. You will not have to call it in functions that you are required to implement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Written Questions [4pts]:\n",
    "\n",
    "For each row of logits we subtract the max, then calculate the logsumexp, then add the max back. In this question, you'll explore why this works.\n",
    "\n",
    "**1. Show (algebraically) how adding the maximum for each row $logit$ will return a correct result.** More formally:\n",
    "\n",
    "Let $l$ be a row of $D$ logits:\n",
    "$$l = \\{l_1, l_2, \\cdots, l_D\\}$$\n",
    "Let $s^*$ be the result of logsumexp applied directly to $l$:\n",
    "$$s^* = \\log \\left( \\sum_{i=1}^D \\exp(l_i) \\right)$$\n",
    "Let $s'$ be the result of logsumexp with our technique, subtracting the max before, adding the max after.\n",
    "$$s' = \\log \\left( \\sum_{j=1}^D \\exp\\left(l_i - \\max_{j\\in[1,D]}[l_j]\\right) \\right) + \\max_{j\\in[1,D]}[l_j]$$\n",
    "\n",
    "You need to show that $s'=s^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFB_8-vj_Mv8"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Why is numeric stability important when implementing functions like softmax and logsumexp in the GMM algorithm? Briefly describe how unstable computations could affect the model’s outputs or training process.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ig0zfoum_Mv8"
   },
   "source": [
    "### 3.1.3. Multivariate Gaussian PDF [5pts]\n",
    "\n",
    "You should be able to write your own function based on the following formula, and you are **NOT allowed** to use outside resource packages other than those we provided.\n",
    "\n",
    "**(for undergrads only) normalPDF**\n",
    "\n",
    "Using the covariance matrix as a diagonal matrix with variances of the individual variables appearing on the main diagonal of the matrix and zeros everywhere else means that we assume the features are independent. In this case, the multivariate normal density function simplifies to the expression below:\n",
    "$$\\mathcal{N}(x: \\mu, \\Sigma) = \\prod_{i=1}^D \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}}\\exp{\\left( -\\frac{1}{2\\sigma_i^2} (x_i-\\mu_i)^2\\right)}$$\n",
    "where $\\sigma^2_i$ is the variance for the $i^{th}$ feature, which is the diagonal element of the covariance matrix.\n",
    "\n",
    "**(for grads only) multinormalPDF**\n",
    "\n",
    "Given the dataset $X \\in \\mathbb{R}^{N \\times D}$, the mean vector $\\mu \\in \\mathbb{R}^{D}$ and covariance matrix $\\Sigma \\in \\mathbb{R}^{D \\times D}$ for a multivariate Gaussian distrubution, calculate the probability $p \\in \\mathbb{R}^{N}$ of each data. The PDF is given by\n",
    "$$\\mathcal{N}(X: \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{D/2}}|\\Sigma|^{-1/2}\\exp{\\left(-\\frac{1}{2}(X-\\mu)\\Sigma^{-1}(X-\\mu)^T\\right)}$$\n",
    "where $|\\Sigma|$ is the determinant of the covariance matrix.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- If you encounter \"LinAlgError\", you can mitigate your number/array by summing a small value before taking the operation, e.g. np.linalg.inv($\\Sigma_k$ + SIGMA_CONST). You can arrest and handle such error by using [Try and Exception Block](https://realpython.com/python-exceptions/#the-try-and-except-block-handling-exceptions) in Python. Please only add `SIGMA_CONST` to all elements when `sigma_i` is not invertible.\n",
    "\n",
    "- In the above calculation, you must avoid computing a $(N,N)$ matrix. Using the above equation for large N will crash your kernel and/or give you a memory error on Gradescope. Instead, you can do this same operation by calculating $(X-\\mu)\\Sigma^{-1}$, a $(N,D)$ matrix, transpose it to be a $(D,N)$ matrix and do an element-wise multiplication with $(X-\\mu)^T$, which is also a $(D,N)$ matrix. Lastly, you will need to sum over the 0 axis to get a $(1,N)$ matrix before proceeding with the rest of the calculation. This uses the fact that doing an element-wise multiplication and summing over the 0 axis is the same as taking the diagonal of the $(N,N)$ matrix from the matrix multiplication.\n",
    "\n",
    "- In Numpy implementation for each individual $\\mu$, you can either use a 2-D array with dimension $(1,D)$ for each Gaussian Distribution, or a 1-D array with length $D$. Same to other array parameters. Both ways should be acceptable but pay attention to the shape mismatch problem and be **consistent all the time** when you implement such arrays.\n",
    "\n",
    "- Please **DO NOT** use `self.D` in your implementation of `multinormalPDF()` .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cj-SyonU_Mv9"
   },
   "source": [
    "### 3.2 GMM Implementation [30pts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVoCLl_3_Mv9"
   },
   "source": [
    "Things to do in this problem:\n",
    "\n",
    "### 3.2.1. Initialize parameters in \\_init_components() [5pts]\n",
    "\n",
    "Examples of how you can initialize the parameters.\n",
    "\n",
    "1. `create_pi()`: Set the prior probability $\\pi$ the same for each class.\n",
    "2. `create_mu()`: Initialize $\\mu$ by randomly selecting K numbers of observations as the initial mean vectors. You should use np.random.choice() with replacement set to True for random selection of K numbers of observations.\n",
    "3. `create_sigma()`: Initialize the covariance matrix with [np.eye()](https://numpy.org/devdocs/reference/generated/numpy.eye.html) for each k. For grads, you can also initialize the $\\Sigma$ by K diagonal matrices. It will become a full matrix after one iteration, as long as you adopt the correct computation.\n",
    "4. You are expected to call these methods in the `_init_components()` method\n",
    "5. The autograder will only test the shape of your $\\pi$,$\\mu$, $\\sigma$. Make sure you pass other evaluations in the autograder.\n",
    "\n",
    "### 3.2.2. Formulate the log-likelihood function \\_ll_joint() [10pts]\n",
    "\n",
    "The log-likelihood function is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\ell(\\theta) = \\sum_{i=1}^N \\text{ln } \\big( \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x^{\\{i\\}} | \\mu_k, \\Sigma_k)\\big)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In this part, we will generate a $(N,K)$ matrix where each datapoint $x^{\\{i\\}}, \\forall i = 1, \\dots, N$ has $K$ log-likelihood numbers. Thus, for each $i = 1, \\dots, N$ and $k = 1, \\dots, K$,\n",
    "\n",
    "$$\n",
    "\\text{log-likelihood}[i,k] = \\log{\\pi_k}+\\log{\\cal{N}(x^{\\{i\\}}|\\mu_k, \\Sigma_k)}\n",
    "$$\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- If you encounter \"ZeroDivisionError\" or \"RuntimeWarning: divide by zero encountered in log\", you can mitigate your number/array by summing a small value before taking the operation, e.g. $\\text{log-likelihood}[i,k] = \\log{(\\pi_k + \\text{1e-32})}+\\log{(\\cal{N}(x^{\\{i\\}}|\\mu_k, \\Sigma_k) + \\text{1e-32})}$. If you pass the local test cases but fail the autograder, make sure you sum a small value like the example we given.\n",
    "- You need to use the Multivariate Normal PDF function you created in the last part. Remember the PDF function is for each Gaussian Distribution (i.e. for each k) so you need to use a for loop over K.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EOWVBev_Mv9"
   },
   "source": [
    "### 3.2.3. Setup Iterative steps for EM Algorithm [5pts + 10pts]\n",
    "\n",
    "You can find the detail instruction in the above description box.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- For E steps, we already get the log-likelihood at \\_ll_joint() function. This is not the same as responsibilities ($\\tau$), but you should be able to finish this part with just a few lines of code by using \\_ll_joint() and softmax() defined above.\n",
    "- For undergrads: Try to simplify your calculation for $\\Sigma$ in M steps as you assumed independent components. Make sure you are only taking the diagonal terms of your calculated covariance matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acABZ2wk_Mv-"
   },
   "source": [
    "### Function Tests\n",
    "\n",
    "Use these to test if your implementation of functions in GMM work as expected. See [Using the Local Tests](#using_local_tests) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:21:47.529862Z",
     "iopub.status.busy": "2025-09-15T02:21:47.529579Z",
     "iopub.status.idle": "2025-09-15T02:21:47.578485Z",
     "shell.execute_reply": "2025-09-15T02:21:47.577186Z"
    },
    "id": "-AYRHI1s_Mv-"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from gmm import *\n",
    "from utilities import plot_images\n",
    "\n",
    "gmm_tester = localtests.GMMTests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-15T02:21:47.581683Z",
     "iopub.status.busy": "2025-09-15T02:21:47.581443Z",
     "iopub.status.idle": "2025-09-15T02:21:47.613303Z",
     "shell.execute_reply": "2025-09-15T02:21:47.612613Z"
    },
    "id": "VMuVVgYn_Mv-",
    "outputId": "ae873429-2d1b-4987-913d-749efe0b4a77"
   },
   "outputs": [],
   "source": [
    "gmm_tester.test_helper_functions()\n",
    "gmm_tester.test_init_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-15T02:21:47.615995Z",
     "iopub.status.busy": "2025-09-15T02:21:47.615594Z",
     "iopub.status.idle": "2025-09-15T02:21:47.647053Z",
     "shell.execute_reply": "2025-09-15T02:21:47.646548Z"
    },
    "id": "eYB4Dpzm_Mv_",
    "outputId": "1be9fc28-dfe5-4809-a341-541c3c7a1e6c"
   },
   "outputs": [],
   "source": [
    "gmm_tester.test_undergrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-15T02:21:47.649462Z",
     "iopub.status.busy": "2025-09-15T02:21:47.649181Z",
     "iopub.status.idle": "2025-09-15T02:21:47.679775Z",
     "shell.execute_reply": "2025-09-15T02:21:47.679042Z"
    },
    "id": "OLsPpJKK_Mv_",
    "outputId": "7ca97408-896d-48c9-c8a4-cf4a949fb91b"
   },
   "outputs": [],
   "source": [
    "gmm_tester.test_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBt48eRG_MwA"
   },
   "source": [
    "## 3.3 Image Compression and pixel clustering [12pts]\n",
    "\n",
    "#### 3.3.1 GMM Clustering in the RGB Space [10pts]\n",
    "\n",
    "Images typically need a lot of bandwidth to be transmitted over the network. In order to optimize this process, most image processors perform lossy compression of images (lossy implies some information is lost in the process of compression).\n",
    "\n",
    "In this section, you will use your GMM algorithm implementation to do pixel clustering and compress the images. That is to say, you would develop a lossy image compression algorithm. This question is autograded based on your GMM implementation. \n",
    "(Hint: you can adjust the number of clusters formed and justify your answer based on visual inspection of the resulting images or on a different metric of your choosing)\n",
    "\n",
    "Implement the `cluster_pixels_gmm` function in gmm.py. Each pixel can be considered as a separate data point (of length 3), which you can then cluster using GMM. Then, process the outputs into the shape of the original image, where each pixel is its most likely value. What do $\\mu$ and $\\tau$ represent?\n",
    "\n",
    "**Special Notes**\n",
    "\n",
    "- Try to add a small value(e.g. SIGMA_CONST and LOG_CONST) before taking the operation if the output image is solid black.\n",
    "- The output images may be slightly different due to different initialization methods in GMM() function.\n",
    "- Sample with replacement in `create_mu`\n",
    "- Undergrads are tested with FULL_MATRIX = False and Grads are tested with FULL_MATRIX = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:21:47.682516Z",
     "iopub.status.busy": "2025-09-15T02:21:47.682294Z",
     "iopub.status.idle": "2025-09-15T02:22:13.436419Z",
     "shell.execute_reply": "2025-09-15T02:22:13.434391Z"
    }
   },
   "outputs": [],
   "source": [
    "gmm_tester.test_undergrad_image_compression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:22:13.444127Z",
     "iopub.status.busy": "2025-09-15T02:22:13.443628Z",
     "iopub.status.idle": "2025-09-15T02:22:38.434552Z",
     "shell.execute_reply": "2025-09-15T02:22:38.433896Z"
    }
   },
   "outputs": [],
   "source": [
    "gmm_tester.test_grad_image_compression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2025-09-15T02:22:38.438059Z",
     "iopub.status.busy": "2025-09-15T02:22:38.437734Z",
     "iopub.status.idle": "2025-09-15T02:24:28.313223Z",
     "shell.execute_reply": "2025-09-15T02:24:28.311834Z"
    },
    "id": "oL9sGASK_MwA",
    "outputId": "7c3601cb-105f-4319-81c1-4d7f073d4035"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "img1_dir = \"./data/images/image_test_1.jpg\"\n",
    "img2_dir = \"./data/images/image_test_2.jpg\"\n",
    "\n",
    "\n",
    "def perform_compression(image, min_clusters=10, max_clusters=20):\n",
    "    for K in range(min_clusters, max_clusters + 1, 5):\n",
    "        gmm_image_k = cluster_pixels_gmm(image, K, max_iters=10, full_matrix=True)\n",
    "        plot_images([image, gmm_image_k], [\"original\", \"gmm=\" + str(K)])\n",
    "\n",
    "\n",
    "image1 = imageio.imread(img1_dir)\n",
    "perform_compression(image1, 10, 15)\n",
    "\n",
    "image2 = imageio.imread(img2_dir)\n",
    "perform_compression(image2, 10, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Written Question [2pts]:\n",
    "\n",
    "After completing the GMM-based image compression, consider how the clustering process would have been different if you had used K-means instead. How do the mixture weights (π) and pixel-cluster responsibilities (τ) in GMM influence the reconstructed image? In what situations might GMM provide advantages over K-means, and when might K-means be sufficient?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfUa5dql_MwB"
   },
   "source": [
    "## 3.4 Compare full covariance matrix with diagonal covariance matrix [1% Bonus for All]\n",
    "\n",
    "Compare the results of clustering an image with full covariance matrix and diagonal covariance matrix. Can you explain why the images are different with same clusters?\n",
    "Note: You will have to implement both multinormalPDF and normalPDF, and add a few arguments in the original \\_ll_joint(), \\_M_step(), \\_E_step() function. **You will earn full credit only if you implement all functions AND provide an explanation.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:24:28.338088Z",
     "iopub.status.busy": "2025-09-15T02:24:28.337721Z",
     "iopub.status.idle": "2025-09-15T02:24:28.824263Z",
     "shell.execute_reply": "2025-09-15T02:24:28.823713Z"
    },
    "id": "qN8YMc3I_MwB"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "\n",
    "def compare_matrix(image, K):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image: input image of shape(H, W, 3)\n",
    "        K: number of components\n",
    "\n",
    "    Return:\n",
    "        plot: comparison between full covariance matrix and diagonal covariance matrix.\n",
    "    \"\"\"\n",
    "    # full covariance matrix\n",
    "    gmm_image_full = cluster_pixels_gmm(image, K, 10, full_matrix=True)\n",
    "    # diagonal covariance matrix\n",
    "    gmm_image_diag = cluster_pixels_gmm(image, K, 10, full_matrix=False)\n",
    "\n",
    "    plot_images(\n",
    "        [gmm_image_full, gmm_image_diag],\n",
    "        [\"full covariance matrix\", \"diagonal covariance matrix\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "execution": {
     "iopub.execute_input": "2025-09-15T02:24:28.827261Z",
     "iopub.status.busy": "2025-09-15T02:24:28.827027Z",
     "iopub.status.idle": "2025-09-15T02:26:42.154546Z",
     "shell.execute_reply": "2025-09-15T02:26:42.153865Z"
    },
    "id": "uVQU4PL4_MwB",
    "outputId": "934bca01-3457-46fe-f8a0-d5cca902978d"
   },
   "outputs": [],
   "source": [
    "compare_matrix(image2, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Generate samples from a Gaussian Mixture [5pts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, you will be fitting your GMM implementation on a 2D Gaussian Mixture to estimate the parameters of the distributions that make up the mixture, and then using these estimated parameters to generate samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `density` and `rejection_sample` functions in gmm.py. Then, fit your GMM implementation below by initializing and calling the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:26:42.178090Z",
     "iopub.status.busy": "2025-09-15T02:26:42.177848Z",
     "iopub.status.idle": "2025-09-15T02:26:42.315384Z",
     "shell.execute_reply": "2025-09-15T02:26:42.314463Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "data = np.load(\"./data/gaussian_clusters.npy\")\n",
    "print(data.shape)\n",
    "\n",
    "plt.plot(data[:, 0], data[:, 1], \"x\")\n",
    "plt.axis(\"equal\")\n",
    "plt.title(\"2-D Gaussian Mixture\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:26:42.318552Z",
     "iopub.status.busy": "2025-09-15T02:26:42.318334Z",
     "iopub.status.idle": "2025-09-15T02:26:42.343497Z",
     "shell.execute_reply": "2025-09-15T02:26:42.342721Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit your GMM implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you need to estimate the parameters of the Gaussian Mixture, and then use these estimated parameters to generate 1000 samples from the Gaussian Mixture. Plot the sampled datapoints. **You should notice that it resembles the original Gaussian Mixture.**\n",
    "\n",
    "**Steps**\n",
    "\n",
    "- First, to estimate the parameters of the Gaussian Mixture, you'll need to fit your GMM implementation to the dataset. You need to specify K=5 to represent 5 gaussians in our model, and run the EM algorithm. You'll have to choose the value for max_iters. If at the end of this section, your plot of the sampled datapoints doesn't look like the original distribution, you may need to increase max_iters to fit the GMM model better, and obtain better estimates of the parameters.\n",
    "- Once you have the estimated parameters, we'll need to sample 1000 datapoints from the Gaussian Mixture. You will be using a technique called Rejection Sampling discussed below. Here are some external sources that may help: https://cosmiccoding.com.au/tutorials/rejection_sampling, https://towardsdatascience.com/rejection-sampling-with-python-d7a30cfc327b\n",
    "  - We will be taking the approach from the first link, but extending it into the 2D space.\n",
    "- The formula for the density function is $f(x^{\\{i\\}}) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x^{\\{i\\}} | \\mu_k, \\Sigma_k)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating vs Sampling**\n",
    "To generate points directly from a given distribution is done via Inverse transform sampling. In inverse transform sampling, we require taking the inverse of cummulative distribution function for our gaussian mixture model. This operation in general can be expensive unless there is some known formula for inverting the CDF. It is also not always possible to take the inverse of the CDF of a gaussian mixture model. For these reasons we will implement a sampling method instead. This sampling method will give us points matching the gmm without the computation and mathematical concerns of generation.\n",
    "\n",
    "**Rejection Sampling**\n",
    "\n",
    "Conventionally we think of Gaussian Mixture Models as a form of soft clustering, but you can also think of them as an algorithm for estimating density of data points with gaussians. Thus we can take an arbitary data point and using the gaussian mixture model as an estimation for the density at a given location. From here we want the points that we sample to be proportional to the density at a given location.\n",
    "\n",
    "We go about this by, choosing an arbitary point (x,y). Then we use the density formula function $f(x^{\\{i\\}}) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x^{\\{i\\}} | \\mu_k, \\Sigma_k)$ to find out what the density of points is at (x,y). Now that we have the density, we can draw a random number between 0 and the maximum density to determine if we will keep or discard (x,y). If the random number drawn is less than the density, then (x,y) is our sample, otherwise we discard (x,y) and repeat. This method ensure that the samples we generate are proportional to the density predicted by our GMM at any given area.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:26:42.454737Z",
     "iopub.status.busy": "2025-09-15T02:26:42.454530Z",
     "iopub.status.idle": "2025-09-15T02:26:42.483798Z",
     "shell.execute_reply": "2025-09-15T02:26:42.482840Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# Extract x and y\n",
    "x = data[:, 0]\n",
    "y = data[:, 1]\n",
    "\n",
    "# Define the borders of the grid\n",
    "deltaX = (max(x) - min(x)) / 10\n",
    "deltaY = (max(y) - min(y)) / 10\n",
    "xmin = min(x) - deltaX\n",
    "xmax = max(x) + deltaX\n",
    "ymin = min(y) - deltaY\n",
    "ymax = max(y) + deltaY\n",
    "\n",
    "\n",
    "# Create meshgrid\n",
    "xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "# coordinates of the points that make the grid\n",
    "positions = np.vstack([xx.ravel(), yy.ravel()]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:26:42.486916Z",
     "iopub.status.busy": "2025-09-15T02:26:42.486699Z",
     "iopub.status.idle": "2025-09-15T02:26:44.446256Z",
     "shell.execute_reply": "2025-09-15T02:26:44.445494Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "\n",
    "# get the density at each coordinate on the grid\n",
    "densities = np.reshape(density(positions, pi, mu, sigma, gmm), xx.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(13, 7), dpi=100)\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "surf = ax.plot_surface(\n",
    "    xx, yy, densities, rstride=1, cstride=1, cmap=\"coolwarm\", edgecolor=\"none\"\n",
    ")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_zlabel(\"PDF\")\n",
    "ax.set_title(\"Surface plot of 2D Gaussian Mixture Densities\")\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)  # add color bar indicating the PDF\n",
    "ax.view_init(60, 35)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:26:44.460797Z",
     "iopub.status.busy": "2025-09-15T02:26:44.460521Z",
     "iopub.status.idle": "2025-09-15T02:26:55.276395Z",
     "shell.execute_reply": "2025-09-15T02:26:55.275625Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "\n",
    "# Sample datapoints using Rejection Sampling\n",
    "generated_datapoints = np.zeros((1000, 2))\n",
    "i = 0\n",
    "while i < 1000:\n",
    "    generated_datapoints[i, 0], generated_datapoints[i, 1] = rejection_sample(\n",
    "        xmin, xmax, ymin, ymax, pi, mu, sigma, gmm, dmax=1\n",
    "    )\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:26:55.279200Z",
     "iopub.status.busy": "2025-09-15T02:26:55.278969Z",
     "iopub.status.idle": "2025-09-15T02:26:55.402723Z",
     "shell.execute_reply": "2025-09-15T02:26:55.402156Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "plt.scatter(generated_datapoints[:, 0], generated_datapoints[:, 1])\n",
    "plt.axis(\"equal\")\n",
    "plt.title(\"Sampled Datapoints\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Random vs. KMeans Initialization [1\\% Bonus for All]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing our GMM parameters randomly could yield a long training time, so we could try to use a heuristic for initialization. For example, we could use a trained KMeans model on our dataset, to initialize the centers (mu) of our GMM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `create_mu_kmeans` function in gmm.py. Then, call it in your `_init_components` function using the `kmeans_init` boolean to denote if you're using the kmeans initialization or random initialization. Finally, the below cells will run a KMeans initialization and a random initialization, and print out the number of iterations GMM takes to converge on both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with the below parameters!\n",
    "# num_trials may be particularly influential to your results\n",
    "#   since the randomness is unseeded, so the intialization techniques may perform uncharacteristically poor or well on this few trials\n",
    "# however, the other parameters may also be poorly conditioned\n",
    "\n",
    "full_matrix = False\n",
    "K = 5\n",
    "max_iters = 100\n",
    "rel_tol = 1e-5\n",
    "num_trials = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:27:09.275712Z",
     "iopub.status.busy": "2025-09-15T02:27:09.275428Z",
     "iopub.status.idle": "2025-09-15T02:27:11.733693Z",
     "shell.execute_reply": "2025-09-15T02:27:11.733134Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "df = sns.load_dataset(\"iris\")\n",
    "data = np.array(df.drop(\"species\", axis=1))\n",
    "\n",
    "random_iters = []\n",
    "kmeans_iters = []\n",
    "\n",
    "for _ in range(num_trials):\n",
    "    student_gmm_random = GMM(data, K, max_iters=max_iters, seed=None)\n",
    "    student_gmm_random(\n",
    "        full_matrix=full_matrix, kmeans_init=False, rel_tol=rel_tol, disable_tqdm=True\n",
    "    )\n",
    "    random_iters.append(student_gmm_random.num_iters)\n",
    "\n",
    "    student_gmm_kmeans = GMM(data, K, max_iters=max_iters, seed=None)\n",
    "    student_gmm_kmeans(\n",
    "        full_matrix=full_matrix, kmeans_init=True, rel_tol=rel_tol, disable_tqdm=True\n",
    "    )\n",
    "    kmeans_iters.append(student_gmm_kmeans.num_iters)\n",
    "\n",
    "print(\"Random num iterations: \", np.mean(random_iters))\n",
    "print(\"KMeans num iterations: \", np.mean(kmeans_iters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate the number of iterations of random initialization against KMeans initialization above. Which initialization allows GMM to converge faster during training? Why do you think KMeans initialization is slower or faster (~1-2 sentences)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhVsVVOA_MwB"
   },
   "source": [
    "## 4. (Bonus for All) Cleaning Messy data and semi-supervised learning [7% Bonus for All]\n",
    "\n",
    "Learning to work with messy data is a hallmark of a well-rounded data scientist. In most real-world settings the data given will usually have some issue, so it is important to learn skills to work around such impasses. This part of the assignment looks to expose you to clever ways to fix data using concepts that you have already learned in the prior questions.\n",
    "\n",
    "#### Problem Scenario\n",
    "\n",
    "_Congratulations! you recently graduated with your shiny GT degree. You've decided to pursue your childhood dream to study astrophysics. Stationed at a cutting edge Gamma Ray Telescope facility as a Data Scientist, delving into the high-energy physics of the universe, your mission is to probe the enigmatic Sagittarius A*, the supermassive black hole in the center of our galaxy, using a telescope that views the cosmos through gamma rays emitted by the most cataclysmic events in space like neutron star mergers, pulsars, and the voracious accretion disks of black holes. SUPER EXCITING!_\n",
    "\n",
    "_The cutting edge telescope you are working with detects these really high energy gamma ray emissions from really small windows in the sky. You find that the telescope can be configured with a few parameters to detect these particles. These parameters, 10 in number, range from the telescope's orientation and timing precision to the sensitivity settings that find the faintest gamma signals against the cosmic background._\n",
    "\n",
    "_However, your cosmic quest faces an unexpected challenge. A data corruption incident has left a 15% void across your dataset, affecting both the intricate telescope parameters and the critical gamma ray detection records. This isn't just a minor hiccup; it's a significant obstacle in your path to unraveling the mysteries of our galaxy's heart._\n",
    "\n",
    "_But there's a silver lining. You remember that the machine learning techniques learnt in CS4641/7641 are the key to navigating this data loss issue. This challenge transforms into an opportunity to showcase the resilience and ingenuity of data science in the face of adversity. How will you leverage your skills to reconstruct the missing pieces and ensure that your exploration of Sagittarius A* yields groundbreaking insights into the universe's most profound secrets?_\n",
    "\n",
    "**Task:** \n",
    "Clean the data and implement a semi-supervised learning framework to classify the detection of gamma rays for your experiments. The data has 10 feature columns containing the telescope's parameters and one column containing a binary label containing either (0 or 1) representing the absence or a presence of a signal.\n",
    "\n",
    "You are given two files for this task:\n",
    "\n",
    "- data.csv: the entire dataset with complete and incomplete data\n",
    "- validation.csv: a smaller, fully complete dataset made after the intern deleted the datapoints\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNi4X5bk_MwB"
   },
   "source": [
    "### 4.1 Data Cleaning [2.8%]\n",
    "\n",
    "### 4.1.a Data Separating [0.7%]\n",
    "\n",
    "The first step is to break up the whole dataset into clear parts. All the data is randomly shuffled in one csv file. In order to move forward, the data needs to be split into three separate arrays:\n",
    "\n",
    "- labeled_complete: containing the complete characterization data and corresponding labels\n",
    "- labeled_incomplete: containing partial characterization data (i.e., one of the features is NaN) and corresponding labels\n",
    "- unlabeled_complete: containing complete characterization data but no corresponding labels (i.e., the label is NaN)\n",
    "\n",
    "In **semisupervised.py**, implement the following methods:\n",
    "\n",
    "- complete\\_\n",
    "- incomplete\\_\n",
    "- unlabeled\\_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/images/dirty_dataset_example.png\" width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:27:11.736668Z",
     "iopub.status.busy": "2025-09-15T02:27:11.736455Z",
     "iopub.status.idle": "2025-09-15T02:27:11.766796Z",
     "shell.execute_reply": "2025-09-15T02:27:11.765900Z"
    },
    "id": "oKXYgzvc_MwC",
    "outputId": "7b99134a-19d8-4759-ea33-bc8aa7b64439"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "localtests.SemisupervisedTests().test_data_separating_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CoW4bOY1_MwC"
   },
   "source": [
    "### 4.1.b KNN [1.4%]\n",
    "\n",
    "The second step in this task is to clean the Labeled_incomplete dataset by filling in the missing values with probable ones derived from complete data. A useful approach to this type of problem is using a [k-nearest neighbors (k-NN) algorithm](https://www.freecodecamp.org/news/k-nearest-neighbors-algorithm-classifiers-and-model-example/). For this application, the method consists of replacing the missing value of a given point with the mean of the closest k-neighbors to that point. Given that you are focusing on neighbouring points, the margin of error from actual missing values should be limited.\n",
    "\n",
    "In the **CleanData** class in **semisupervised.py**, implement the following methods:\n",
    "\n",
    "- pairwise_dist\n",
    "- \\_\\_call\\_\\_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paq2V-yu_MwC"
   },
   "source": [
    "The unit test is a good expectation of what the process should look like on a toy dataset. If your output matches the answer, you are on the right track. Run the following cell to check.\n",
    "\n",
    "NOTE: Your rows of data should match with the expected output, although the order of the rows does not necessarily matter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:27:11.770028Z",
     "iopub.status.busy": "2025-09-15T02:27:11.769780Z",
     "iopub.status.idle": "2025-09-15T02:27:11.800415Z",
     "shell.execute_reply": "2025-09-15T02:27:11.799780Z"
    },
    "id": "PZ0M7oKV_MwC",
    "outputId": "e918a1e2-206e-44f2-ccf7-f17b3bf20fc8"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "localtests.SemisupervisedTests().test_cleandata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.c Median of Features [0.7%]\n",
    "\n",
    "Another method of filling the missing values is by using the median of individual features. Our goal with replacing NaN values is to insert values in their place while also minimally disturbing the overall distribution of each feature. Using the median of features helps avoid drastically changing the distribution of our data. This is also why while we could technically replace NaN values with 0, it is generally not advised to do so.\n",
    "\n",
    "Implement the median_clean_data method in accordance with this rule.\n",
    "NOTE: There should be no NaN values in the n\\*d array that you return from median_clean_data.\n",
    "\n",
    "In **semisupervised.py**, implement the following method:\n",
    "\n",
    "- median_clean_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:27:11.803039Z",
     "iopub.status.busy": "2025-09-15T02:27:11.802790Z",
     "iopub.status.idle": "2025-09-15T02:27:11.833445Z",
     "shell.execute_reply": "2025-09-15T02:27:11.832755Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "localtests.SemisupervisedTests().test_median_clean_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xqpO5wU_MwD"
   },
   "source": [
    "### 4.2 Semi-supervised Learning [3.5%]\n",
    "\n",
    "Semi-supervised learning is a machine learning approach that bridges the gap between supervised and unsupervised learning by training models on a dataset containing both labeled and unlabeled data. Typically, only a small portion of the dataset is labeled often because labeling is expensive, time consuming, or requires expert knowledge while the majority remains unlabeled. Despite the scarcity of labeled data, semi-supervised learning leverages it to provide initial guidance, then uses patterns and structures within the unlabeled data to further refine and enhance the model’s performance. This method is particularly effective in real-world scenarios where collecting large amounts of labeled data is impractical but raw data is abundant, such as in medical diagnosis, speech recognition, and natural language processing. A notable application is in text classification tasks, where a few annotated examples can be used to improve classification accuracy across large corpora of unlabelled text, as demonstrated in the paper discussed in the next section.\n",
    "\n",
    "### 4.2.a Getting acquainted with semi-supervised learning approaches. [1.2%]\n",
    "\n",
    "Take a look at the algorithm presented in Table 1 of the paper [\"Text Classification from Labeled and Unlabeled Documents using EM\"](http://www.kamalnigam.com/papers/emcat-mlj99.pdf) by Nigam et al. (2000). While you are recommended to read the whole paper this assignment focuses on items 5.1, 5.2, and 6.1. Write a brief summary of three interesting highlights of the paper (50-words maximum).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47oHreu7_MwD"
   },
   "source": [
    "### 4.2.b Implementing the EM algorithm. [2.3%]\n",
    "\n",
    "Implement the EM algorithm proposed by Nigam et al. (2000) on Table 1, using a Gaussian Naive Bayes (GNB) classifier instead of a Naive Bayes (NB) classifier. What's the difference between the way of initialization in the paper and the way introduced in class?\n",
    "\n",
    "(Hint: Using a GNB in place of an NB will enable you to reuse most of the implementation you developed for GMM in this assignment. Instead of building an initial naive Bayes classifier like it says in the second bullet point of Table 1, think about how we can implement this step with Gaussians. In fact, you can successfully solve the problem by simply modifying the \\_\\_call\\_\\_ and \\_init_components methods.)\n",
    "\n",
    "In the **SemiSupervised** class in **semisupervised.py**, implement the following methods:\n",
    "\n",
    "- \\_init_components\n",
    "- \\_\\_call\\_\\_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vmUAYXN_MwD"
   },
   "source": [
    "### 4.3 Demonstrating the performance of the algorithm [0pts]\n",
    "\n",
    "Let's compare the classification error based on the Gaussian Naive Bayes (GNB) classifier you implemented following the Nigam et al. (2000) approach to the performance of a GNB classifier trained using only labeled data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:27:11.836463Z",
     "iopub.status.busy": "2025-09-15T02:27:11.836124Z",
     "iopub.status.idle": "2025-09-15T02:27:13.000401Z",
     "shell.execute_reply": "2025-09-15T02:27:12.999856Z"
    },
    "id": "25JoKgo7_MwD"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "from semisupervised import ComparePerformance\n",
    "\n",
    "ComparePerformance.accuracy_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Interpretation of Results. [0.7%]\n",
    "\n",
    "What are the differences in using the kNN method and the median method to fill NaN values? Explain in terms of the results you get from each. What would be some advantages of using the median method to fill in NaN values over using the **mean** of features?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Hierarchical Clustering [9 pts Grad / 4.5% Bonus for Undergrad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Hierarchical Clustering Implementation [9pts Grad/4.5% Bonus for Undergrad]\n",
    "\n",
    "Hierarchical Clustering is a bottom-up agglomerative clustering algorithm which iteratively combines the closest pair of clusters. Each point starts off as its own cluster, and in each iteration you'll find the closest clusters and update the distances to the new cluster using single-link clustering, keeping track of the order in which the clusters are combined. In this section, you'll implement the `create_distance_matrix`, `iterate`, and `fit` methods in **hierarchical_clustering.py**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `HierarchicalClustering` class has several instance variables that you may need to create and update in each iteration:\n",
    "1. `points`: N x D numpy array where N is the number of points, and D is the dimensionality of each point. This is your dataset.\n",
    "2. `distance`: N x N symmetric numpy array which stores pairwise distances between clusters. The distance between a cluster and itself should be `np.inf` in order to help us calculate the closest pair later\n",
    "3. `cluster_ids`: (N,) numpy array where index_array[i] gives the cluster id of the i-th column and i-th row of distances. Initially, each point with index `points[i, :]` is assigned cluster id i, and new points are assigned cluster ids starting from `N` and incrementing.\n",
    "4. `clustering`: (N - 1, 4) numpy array that keeps track of which clusters were merged in each iteration. `clustering[iteration_number]` keeps track of the first cluster id, second cluster id, distance between first and second clusters, and the size of new cluster\n",
    "5. `cluster_sizes` (2N - 1, ) numpy array that stores the number of points in each cluster, indexed by id. Because there are `N` original clusters corresponding to each point, and each iteration merges two clusters, there will be `2N-1` total clusters created. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theses are the following functions you'll have to implement in **hierarchical_clustering.py**:\n",
    "\n",
    "1. `create_distances`: Creates the initial distance matrix and cluster ids\n",
    "2. `iterate`: Merges the two closest clusters\n",
    "3. `fit`: Calls `iterate` multiple times and returns the clusterings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of how the instance variables should be updated in an iteration is shown below:\n",
    "\n",
    "Before:\n",
    "\n",
    "<img align=\"left\" src=\"data/images/ml hw2 hierarchical clustering example/Slide1.png\" width=\"400\">\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;\n",
    "current_iteration = `0`\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;\n",
    "cluster_ids = `[0, 1, 2, 3]`\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;\n",
    "cluster_sizes = `[1, 1, 1, 1, 0, 0, 0]`\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;\n",
    "clustering = `[[0, 0, 0, 0], …, [0, 0, 0, 0]]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calling `iterate`\n",
    "\n",
    "<img align=\"left\" src=\"data/images/ml hw2 hierarchical clustering example/Slide2.png\" width=\"400\">\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;\n",
    "current_iteration = `1`\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;\n",
    "cluster_ids = `[4, 1, 3]`\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;\n",
    "*The first row/col represents the cluster with id 4 (the new cluster), the second row/col represents the cluster with id 1, and the third row/col represents the cluster with id 3. Clusters with ids 0 and 2 are deleted from the distance matrix after being combined*\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;\n",
    "cluster_sizes = `[1, 1, 1, 1, 2, 0, 0]`\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;\n",
    "clustering = `[[0, 2, 1, 2], …, [0, 0, 0, 0]]`\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;\n",
    "*For the first iteration, we combined cluster ids 0 and 2, which had an inter-cluster distance of 1 and the new cluster contains 2 points*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:27:13.003931Z",
     "iopub.status.busy": "2025-09-15T02:27:13.003650Z",
     "iopub.status.idle": "2025-09-15T02:27:13.031138Z",
     "shell.execute_reply": "2025-09-15T02:27:13.030333Z"
    }
   },
   "outputs": [],
   "source": [
    "from hierarchical_clustering import HierarchicalClustering\n",
    "from scipy.cluster import hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:27:13.034009Z",
     "iopub.status.busy": "2025-09-15T02:27:13.033783Z",
     "iopub.status.idle": "2025-09-15T02:27:13.066620Z",
     "shell.execute_reply": "2025-09-15T02:27:13.065983Z"
    }
   },
   "outputs": [],
   "source": [
    "localtests.HierarchicalClusteringTests().test_create_distance()\n",
    "localtests.HierarchicalClusteringTests().test_iterate_1d()\n",
    "localtests.HierarchicalClusteringTests().test_iterate_2d()\n",
    "localtests.HierarchicalClusteringTests().test_fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Hierarchical Clustering Visualization [0 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you'll run Hierarchical Clustering on an example dataset and visualize it in a dendrogram using SciPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:27:13.069736Z",
     "iopub.status.busy": "2025-09-15T02:27:13.069506Z",
     "iopub.status.idle": "2025-09-15T02:27:13.096207Z",
     "shell.execute_reply": "2025-09-15T02:27:13.095584Z"
    }
   },
   "outputs": [],
   "source": [
    "points = np.array([[5, 5], [1, 1], [5, 6], [8, 8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:27:13.098790Z",
     "iopub.status.busy": "2025-09-15T02:27:13.098567Z",
     "iopub.status.idle": "2025-09-15T02:27:13.287295Z",
     "shell.execute_reply": "2025-09-15T02:27:13.286508Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "hc = HierarchicalClustering(points)\n",
    "clustering = hc.fit()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "axes[0].scatter(points[:, 0], points[:, 1], c=[\"blue\", \"red\", \"green\", \"black\"])\n",
    "for i in range(points.shape[0]):\n",
    "    axes[0].annotate(i, points[i] + 0.1)\n",
    "    axes[0].set_title(\"Points\")\n",
    "\n",
    "hierarchy.set_link_color_palette([\"m\", \"c\", \"y\", \"k\"])\n",
    "dn1 = hierarchy.dendrogram(clustering, ax=axes[1], orientation=\"top\")\n",
    "axes[1].set_title(\"Generated dendrogram\")\n",
    "hierarchy.set_link_color_palette(None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Hierarchical Clustering Large Dataset Visualization [0 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you'll run Hierarchical Clustering on a larger dataset (Iris). Run the following code cell once in order to install the library that will allow you to visualize a radial dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import radialtree as rt\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from seaborn import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a simple dataset\n",
    "iris = load_dataset(\"iris\")\n",
    "species = iris.pop(\"species\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(20, 15))\n",
    "\n",
    "# Compute and plot the dendrogram.\n",
    "Y = sch.linkage(np.asarray(iris), method=\"average\")\n",
    "Z2 = sch.dendrogram(\n",
    "    Y,\n",
    "    # no_plot=True,\n",
    "    ax=axes[0],\n",
    "    color_threshold=1.0,\n",
    ")\n",
    "\n",
    "axes[1].set_aspect(1)\n",
    "# plot a circular dendrogram\n",
    "rt.radialTreee(Z2, ax=axes[1], sample_classes={\"species\": species})"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "xuEnlmN4_Mv5",
    "OYMGmHnb_Mv5",
    "AsQ451om_Mv6",
    "F6RKWYnT_Mv6",
    "B5lmW2uC_Mv6",
    "fKoOWjNL_Mv6",
    "vEH3Rq6I_Mv7",
    "Ig0zfoum_Mv8",
    "Cj-SyonU_Mv9",
    "nVoCLl_3_Mv9",
    "7EOWVBev_Mv9",
    "XNi4X5bk_MwB",
    "CoW4bOY1_MwC",
    "3xqpO5wU_MwD",
    "47oHreu7_MwD",
    "-vmUAYXN_MwD"
   ],
   "name": "SUMMER2022_HW2_Solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
